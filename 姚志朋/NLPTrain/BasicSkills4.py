##åˆ©ç”¨CKIP Transformersé€²è¡Œæƒ…æ„Ÿåˆ†æ
train_rows = [
 [[1, 0, 0],
  "ä¹¾æ·¨ä¸å—æ‰“æ“¾ã€‚æ²’æœ‰æ«ƒå°äººå“¡æ˜¯ååˆ†ä¿¡èªé¡§å®¢çš„åœ°æ–¹ï¼Œé›–ç„¶æˆ–è¨±å°‘äº†é»å™“å¯’å•æš–çš„è¦ªåˆ‡ï¼Œå»ä¹Ÿå¤šäº†ä¸€ä»½ä¸å—ç›£è¦–çš„è‡ªåœ¨æ„Ÿã€‚"],
 [[1, 0, 0],
  "æ°‘å®¿è€é—†è·Ÿè€é—†å¨˜è¦ªåˆ‡ã€‚èŒ¶æ°´é–“é™„è¨­è¨±å¤šé›»å™¨åŠæ³¡éºµï¼Œå¾ˆå¤šæ°‘å®¿æ²’æœ‰é€™æ¨£çš„æœå‹™"],
 [[1, 0, 0],
  "ç’°å¢ƒèˆ’é©ï¼Œæˆ¿é–“å¾ˆå®‰éœï¼Œå…’ç«¥éŠæ¨‚ç‰©å“å¤šï¼Œè€é—†å¤«å¦»å¾ˆè¦ªåˆ‡äººå¾ˆå¥½ï¼Œå¾ˆå„ªè³ªã€‚"],
 [[1, 0, 0],
  "æºœæ»‘æ¢¯æˆ¿è¨­è¨ˆå¾ˆå¥½å­©å­å¾ˆå–œæ­¡æ°‘å®¿è€é—†è€é—†å¨˜å¾ˆå‹å–„ğŸ‘"],
 [[1, 0, 0],
  "è€é—†ã€è€é—†å¨˜éƒ½å¾ˆè¦ªåˆ‡ï¼Œé‚„æ¨è–¦å¾ˆå¤šå¯ä»¥å»ç©çš„åœ°æ–¹ï¼ŒçœŸçš„å¾ˆä¸éŒ¯å”·ï¼"],
 [[1, 0, 0],
  "é©åˆè¦ªå­ä½å®¿ä¸”å°æœ‹å‹ä½åˆ°æ¨ä¸å¾—é›¢é–‹,å•†åœˆç†±é–™æ¡è²·æ–¹ä¾¿,ä¸‹æ¬¡é‚„æœƒä¾†ä½!"],
 [[1, 0, 0],
  "å…’ç«¥éŠæˆ²å€åŠå¬°å…’é…å¥—"],
 [[1, 0, 0],
  "è‚²å…’ç”¨å“å¹¾ä¹å…¨éƒ¨éƒ½æœ‰éå¸¸æ–¹ä¾¿ï¼Œç’°å¢ƒèˆ’é©æ•´æ½”ï¼Œåœ°é»æ–¹ä¾¿ã€‚æˆ¿é–“å…§æœ‰æºœæ»‘æ¢¯å¯ç©ï¼Œè€Œä¸”å®¹æ˜“æ’åˆ°çš„åœ°æ–¹éƒ½æœ‰è²¼é˜²æ’æ¢éå¸¸ç´°å¿ƒï¼Œä¹Ÿæœ‰æ•´é½Šä¹¾æ·¨çš„å…¬å…±éŠæˆ²å®¤ã€‚è¦åœè»Šå¯äº‹å…ˆé ç´„ã€‚å¦‚æœæœ‰å¸¶å°å­©ä¾†èŠ±è“®ç©éå¸¸æ¨è–¦ä½é€™è£¡ã€‚"],
 [[1, 0, 0],
  "æˆ¿é–“çš„æºœæ»‘æ¢¯è®“å°å­©é–‹å¿ƒæ¥µäº†ï¼Œå…¶ä»–çš„å…’ç«¥éŠæˆ²é–“åŠé›»å‹•è»Šä¹Ÿéƒ½å¾ˆå—æ­¡è¿ï¼Œç‚ºå…’ç«¥æº–å‚™çš„æ²æµ´ç”¨å“é¸ç”¨Feesä¹Ÿç›¸ç•¶ç”¨å¿ƒï¼Œæ•´é«”éƒ½å¸ƒç½®å¾—ç›¸ç•¶åˆ¥ç·»è€Œä¸”ä¹¾æ·¨"],
 [[1, 0, 0],
  "åºŠå¾ˆèˆ’æœç’°å¢ƒå¾ˆä¹¾æ·¨ä¸»äººå®¶å¾ˆè¦ªåˆ‡å°å°å­©å¾ˆé–‹å¿ƒäº¤é€šæ–¹ä¾¿æ©Ÿèƒ½å¾ˆå¥½å¾ˆæ£’çš„ä½å®¿ç¶“é©—"],
 [[0, 0, 1],
  "å¦‚æœåƒ¹æ ¼å¯ä»¥åœ¨ä¾¿å®œä¸€é»é»æœƒæ›´å¥½."],
 [[0, 0, 1],
  "é›»è¦–ä½ç½®ä¸å¥½ï¼Œä¸èƒ½æ­£é¢çœ‹åˆ°é›»è¦–"],
 [[0, 0, 1],
  "åºŠé‚Šçš„ç‰†ç”¨é˜²æ’ç‰†ç´™æœƒæ›´å¥½ï¼Œå› ç‚ºB Bæ’äº†å¾ˆå¤šæ¬¡"],
 [[0, 0, 1],
  "æºœæ»‘æ¢¯ä¸Šé‹ªçš„åœ°æ¯¯æœ‰äº›ä¸é©åˆå®¹æ˜“å—å‚·ï¼ŒåºŠé‹ªä¸Šæ–¹çš„æºœæ»‘æ¢¯å¤§äººå®¹æ˜“æ’åˆ°é ­æˆ–è¨±å¯ä»¥åŠ å€‹é‚Šè§’ä¿è­·å¢Š?"],
 [[0, 0, 1],
  "æ²’æœ‰ã€‚"],
 [[0, 0, 1],
  "éš”éŸ³ç¨å·®"],
 [[0, 0, 1],
  "æ²’æœ‰"],
 [[0, 0, 1],
  "ç‰†å£ä¸Šæ²’æœ‰æ›é‰¤å¯ä»¥æ›è¡£æœ"],
 [[0, 0, 1],
  "æ²’æœ‰è¡£æ«ƒï¼Œè¡£æœå¤šï¼Œç„¡æ³•æ›"],
 [[0, 0, 1],
  "æ—©é¤æ²’æœ‰é¸æ“‡æ€§,å£å‘³ä¸€èˆ¬,æœ‰é»å¯æƒœ"],
]

from sklearn import metrics
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel, BertConfig, BertTokenizerFast
import numpy as np
import pandas as pd


numpy_data = np.array(train_rows)
numpy_data = pd.DataFrame(data=numpy_data)
trainset, testset = train_test_split(numpy_data, test_size=0.2, random_state = 2022)
trainset, testset = trainset.reset_index(), testset.reset_index()

class CustomDataset(Dataset):

    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.title = dataframe[1]
        self.targets = dataframe[0]
        self.max_len = max_len

    def __len__(self):
        return len(self.title)

    def __getitem__(self, index):
        title = str(self.title[index])
        title = " ".join(title.split())

        inputs = self.tokenizer.encode_plus(
            title,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]


        return {
            'input_ids': torch.tensor(ids, dtype=torch.long),
            'attention_mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'labels': torch.tensor(self.targets[index], dtype=torch.float)
        }
    
    def samples(self):
      return [(title[:self.max_len], target) for target,title in zip(self.targets, self.title)]



#
max_seq_length = 75
tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese',max_length=max_seq_length,padding='max_length', truncation=True)
training_set = CustomDataset(trainset, tokenizer, max_seq_length)
validation_set = CustomDataset(testset, tokenizer, max_seq_length)

from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch

# Define model
def model_init():
  return AutoModelForSequenceClassification.from_pretrained(
      "ckiplab/bert-base-chinese", 
      num_labels=3,
      output_attentions = False, # Whether the model returns attentions weights.
      output_hidden_states = False,
      return_dict=True,
      id2label={0: 'æ­£é¢', 1: 'ä¸­æ€§', 2: 'è² é¢'   }
  )

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=15,
    weight_decay=0.01,
    logging_steps=10,
    save_steps=5,
    seed=2022,
    data_seed=2022,
    evaluation_strategy="steps",
    eval_steps=5

)

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=training_set,
    eval_dataset=validation_set,
    tokenizer=tokenizer,
    data_collator=data_collator

)

trainer.train()